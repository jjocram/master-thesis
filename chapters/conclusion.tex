\documentclass[../thesis.tex]{subfiles}
\begin{document}
\chapter{Conclusion}\label{cap:conclusion}
Through the work done in this thesis, a novel framework for human robot interaction has been proposed, implemented with Python, tested in a Gazebo simulation, and documented to enable other people to use and improve it. The idea behind the framework is to use the hand to control a robot. This was successfully achieved by exploiting MediaPipe to track hands in real-time and two deep neural networks to classify which hand gesture the user is doing by exploiting the coordinates of the hand's conjunctions obtained from MediaPipe. Moreover, the framework handles the communication with the robot through the \gls{ROS} framework. In this way, the developed framework can communicate with any kind of robot, as long as it is compatible with \gls{ROS}.\\

Thanks to the combination of MediaPipe and light deep neural networks (less than ten layers), the training process is really fast. Moreover, the addition of a new gesture is really easy and can be done in a couple of minutes. This is feasible for both static hand gestures and dynamic hand gestures.\\

The possibility to add new gestures ``to the need'' opens the framework to several scenarios. To handle all the possible input sequences and related actions for the robot to perform, the framework asks the user to declare a finite state automaton through a configuration file whose structure was made as simple as possible. In this way, anyone can relate a sequence of gestures to actions performed by the robot.\\

As a proof-of-concept of what is possible to achieve with this framework, it has been tested in a warehouse environment. The \gls{ASL} has been used as static hand gestures to recognize and six gestures taken from the literature have been used as dynamic hand gestures to recognize. Thanks to the framework's integration with Nav2 as the navigation system and \gls{ROS}' topics, a finite state automaton has been described to achieve several tasks, for example, going to a position, picking up a parcel, and dropping off the parcel. The simulation has been done with Gazebo and the AWS small warehouse as ``Gazebo world''.\\

In addition, a way to describe macros has been implemented. In this way, a user can pre-record a sequence of gestures and run it at another moment or edit it with any text editor. The correctness of the sequence is enforced by the same automaton that enforces the correctness of the user input in the real-time scenario.\\

Finally, several tests have been performed to evaluate the quality of the implementation. Those concerning system resource utilization, in particular, are encouraging in terms of a possible deployment on real hardware such as an NVIDIA Jetson Nano or a Raspberry-Pi, two boards widely used in the robot and automation ecosystem.

\end{document}