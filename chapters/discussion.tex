\documentclass[../thesis.tex]{subfiles}
\begin{document}
\chapter{Discussion}\label{cap:discussion}

\section{Hand gestures}
To choose the dynamic hand gestures to use I took inspiration from the literature. I tried to invent my own gestures but, without the possibility to perform a preliminary study involving people from the context of use it would have resulted in unrealistic and unusable gestures in the real world.\\

Regarding the static hand gestures, the choose to use the \acrshort{ASL} seemed an obvious choice to me because it is the common way to express letters using hands in the societies where the Latin alphabet is used.

\section{Deep learning models}
The results achieved with the deep learning models to recognize the hand gestures are astonishing. The choice to use MediaPipe has proven to be a winning one. While I was designing the \acrshort{ML} part of the project I took in consideration the idea to use a YOLO network to recognize the hand gestures because it is known as one of the best network to work on images and videos. But, the downside of this choice would have been the necessity of a big dataset to train the network. On the Internet there are a lot of projects which use YOLO to recognize hand gesture\footnote{\href{https://www.kaggle.com/search?q=hand+gesture}{https://www.kaggle.com/search?q=hand+gesture}} but none of them is easily adaptable on different scenarios or, more simply, it is very difficult to add a new gesture. Moreover, YOLO utilizes the images or sequences of them in the case of video and this is another drawback because when training on images the network has to manage the environment (i.e. where the image is taken), the lightning, and also the skin color.\\

The method utilized (i.e. MediaPipe's landmarks coordinates plus a quite simple feed forward neural network) leaves all the explained above drawbacks to MediaPipe engineers because it uses only the landmarks' coordinates (i.e. numbers) to classify the different hand gestures. Obviously, MediaPipe could be substituted with another \acrshort{ML} model capable of perform hand tracking and returning the coordinates of the conjunctions of the hand but, looking also at the results obtained it seems to be a very good solution.

\subsection{Static hand gestures recognizer}
The feed forward deep neural network used to classify the static hand gestures turned out to be a very good solution with an accuracy $> 99\%$ and a time of only $6$ seconds to train it. Moreover, the results show that it works even with a very small dataset. Furthermore, the time to add a new gesture an train the network is very low.  

\subsection{Dynamic hand gestures recognizer}
The two kind of networks tested: the feed forward network and the \acrshort{LSTM} network achieved similar results. I expected that the later would performed better than the former because the \acrshort{LSTM} network should achieve better results in the context where past data matters. Probably, a difference could not be noticed because a too short sequence of landmarks have been used to train the networks.\\

Moreover, the results shows that more landmarks are taken in consideration more accurate is the prediction. When the tip of all fingers are used the network achieves an accuracy of $98\%$. Instead with just the tip of the index finger the accuracy is $93\%$ and the difference increases if the dataset size is reduced. Furthermore, the time spent to train the network is doubled when the \textit{one\_finger} dataset is used. 

\section{Integration with ROS}
The requirement to work with \acrshort{ROS} has been fulfilled. Moreover the implementation allows other users to expand the capability of the framework. Up to know it can work with Nav2 to guide the robot to a position and send a any type of message to the robot utilizing the \acrshort{ROS} topic. In the future the framework could be expanded making it compatible with other packages and more \acrshort{ROS}' built-ins.

\section{Resource utilization}
The data in~\ref{ss:system_resource_usage_training} shown that the training process is fast and not resource demanding. The RAM usage is just the $2\%$ of $16GB$. Meanwhile, the CPU usage is around the $14\%$ in the case of the static hand gesture recognizer and the $8\%$ in the case of the dynamic hand gesture recognizer.\\

The data gathered during the simulations about the system resource utilization shows that the framework is not resource demanding. Al the simulations have been ran in a virtual machine with $4$ CPU core and $8GB$ of RAM. Regarding the CPU, the results show that after the initialization (the first spike in figure~\ref{fig:cpu_usage}) the utilization is around the $12\%$ and the $14\%$. While, as far as RAM is concerned, its occupation stabilizes below $7\%$ and thus for a total of $~560MB$.\\

Looking at the results, the NVIDIA's Jetson Nano with its $4GB$ of RAM, a four core CPU and the GPU could be take into consideration as a possible hardware to test the framework in a real environment. Maybe also one of the latest Raspberry-Pi could handle it. This is an important fact because those two micro-computers are among the most widely used for the development of robotic application. 
 
\section{Complexity score}
As explained in~\ref{sss:automaton_methodology}, there is not a common way to evaluate the complexity of a JSON file. The method adopted has been chosen to give an objective evaluation of the difficulty of writing and reading a JSON file to meet the non-functional requirement of being easily configurable.
Therefore, it is important to have a low complexity score obtained. To achieve this goal, it is important to wisely design the automaton that describes the accepted input. For example, one can verify that the represented automaton is in its minimized version. Which the automaton obtains from the configuration is.

\end{document}