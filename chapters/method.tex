\documentclass[../thesis.tex]{subfiles}
\begin{document}
\chapter{Method}\label{cap:methods}
To fulfill the requirements of the project I adopted an experimental approach. First and foremost, I was able to determine which tools would have been the most appropriate for doing the computer vision and machine learning tasks. Then, to experiment the solution proposed I designed and developed a proof of concept capable of recognize and learn user's hand gestures, and communicate to a robot which action to perform based on the gesture recognized.
\section{Choice of technologies and tools}\label{sec:technologies_and_tolls}
\subsection{Python}
Python is a high level programming language. It is a well known and highly supported programming language for machine learning tasks. Moreover, it is also supported by \acrshort{ROS} whose documentation has sections written for it. The version used for this project is 3.10 because the pattern matching offered through the \texttt{match ... case} statement is exploited.

\subsection{Tensorflow}
Tensorflow is an open-source Python library to build \acrshort{ML} models. Google started its development in 2015 and today is one of the most used Python libraries to perform \acrshort{ML} tasks. It offers a huge number of layers, activation functions, and tools to build simple and complex neural network architectures. the best known counterpart is PyTorch. During my studies, I have had occasion to use both and I think Tensorflow is more suitable to develop neural networks oriented towards an application. Instead, PyTorch is more appropriate in the development of new and complex neural networks. Moreover, Tensorflow is better integrated with data collections tools like Tensorboard.

\subsubsection{Tensorflow lite}
Tensorflow lite is a component of Tensorflow that allows to convert a Tensorflow model into a compressed flat buffer and then deploy it into any device (e.g.\ mobile devices or embedded device). With the goal of deploying the hand gesture recognizer to a Nvidia Jetson, the use of this tool is natural.

\subsubsection{Keras}
Keras takes advantages of Tensorflow to give to the user a powerful API to design and develop Deep Neural Networks. With a few lines of code is possible to implement complex neural networks that exploit the latest researches in the field of \gls{ML}.

\subsection{MediaPipe}\label{sec:mediapipe}
MediaPipe is an open-source, real-time, and on-device tool that can track multiple parts of the body. In particular, I am interested in hand tracking. MediaPipe suits very well for this purpose because it offers a pre-trained \acrshort{ML} model to recognize and track twenty-one landmarks on each hand (figure~\ref{fig:landmarksMediapipe}). In particular, it uses a pipeline composed of two \acrshort{ML} models:
\begin{enumerate}
    \item A palm detector that works on a full image locates the palm and identifies the bounding box around it;
    \item A hand landmark model that works on the cropped image of the palm and returns the hand landmarks considering the depth also. 
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{thesis/images/mediapipeHandLandmarks.png}
    \caption{Landmarks on a hand recognized by MediaPipe~\cite{site:mediapipe}}
    \label{fig:landmarksMediapipe}
\end{figure}
The precision of this tool is about $96\%$~\cite{paper:mediapipe} so, it is a good starting point for the hand gesture recognition task. It is possible to get the position of the landmarks and give them in input at a Deep Neural Network trained on the gestures of our interest.\\
What MediaPipe returns is the list of coordinates relatives at the 2.5D position of hand's landmarks. These coordinates can be saved and used to train a deep neural network instead images, reducing the size of the dataset.\\
It is interesting to point out that MediaPipe is capable to track, in real-time, different parts of a human body, for example, the face and the whole body.~\cite{site:mediapipe}

\subsection{OpenCV}
OpenCV is an open-source library that fully meets the necessary requirements regarding computer vision and works well with MediaPipe. It is also distributed as a Python package to integrate in in users' applications.

\subsection{Robot Operating System}
\acrfull{ROS} is an open-source set of libraries and tools to develop robot applications. The latest release, and the one used for this project, is \textit{ROS Galactic Geochelone}. The key points to understand when using \acrshort{ROS} are related to the ``ROS 2 Graph'', and they are:
\begin{itemize}
    \item \textbf{Node}: it is a component of the system responsible for a task. It is an executable, in my case a Python executable but it could also be a C++ executable, and it can communicate with other nodes exchanging messages. A robotic system is composed of multiple nodes;
    \item \textbf{Message}: what nodes use to exchange data. Each message has its own ``type'', and this brings advantages in building an interchangeable system because it is possible to change the components of the system with others that can understand the same messages. Messages, in this case, can be seen as the interfaces for programming languages. There are three ways for nodes to exchange messages:
        \begin{itemize}
            \item \textbf{Topic};
            \item \textbf{Service};
            \item \textbf{Action}.
        \end{itemize}
\end{itemize}
\subsubsection{Exchange messages}
\paragraph{Topic}
It is an implementation of the publisher/subscriber pattern. A node can publish messages with a topic, and every other node that is listening to that topic will read that message. This is an asynchronous way to exchange messages because the sender will not know if the message has been read for that this is the best way to broadcast a message without saturating the network.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\columnwidth]{thesis/images/topicExample.png}
    \caption{Example of message exchange between three nodes using a topic.}
    \label{fig:exampleTopicExchangeMessage}
\end{figure}

\paragraph{Service}
It is based on a call-response model. In this case, a node requests some data to another node through a request message. The latter replies with a response message. This is a synchronous way to exchange messages. The node that needs the data waits for the response. There can be many nodes that use the same service to request some data from another node. This is a bit like what happens in a client-server architecture but it should not be used for long running process.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{thesis/images/serviceExample.png}
    \caption{Example of messages exchange between three nodes using a service.}
    \label{fig:exampleServiceExchangeMessage}
\end{figure}

\paragraph{Action}
It uses both topics and services. The functionality is similar to service with the addition of a constant stream of updates from the ``server'' through a topic at which the ``client'' subscribes. The sequence of actions is the following:
    \begin{enumerate}
        \item A node (i.e.\ the client) sends a message (the request for a goal) through a service to another node (i.e.\ the server). The latter replies with one message through the service. For example, it can reply with an acknowledgment or a message saying it starts working on a task;
        \item The server keeps the client updated with the progress of the task through a topic;
        \item The client sends a request through another service to the server. When the task finishes, the server will reply to the client.
    \end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{thesis/images/actionExample.png}
    \caption{Example of messages exchange between two nodes using an action.}
    \label{fig:exampleActionExchangeMessage}
\end{figure}

\subsubsection{Communication between nodes over the network}
\gls{ROS} uses \glsfirst{DDS} as an end-to-end middleware to exchange messages:  This difference is one of the mains between \gls{ROS} version 1 and \gls{ROS} version 2. \gls{DDS} was chosen by \gls{ROS} maintainer for its reliability and flexibility in mission-critical systems, such as:
\begin{itemize}
    \item battleships;
    \item large utility installations;
    \item financial systems;
    \item space systems;
    \item flight systems;
    \item train switchboard systems.
\end{itemize}
The implementation of \gls{DDS} is hidden from the user, who uses the three methods described above with the \gls{ROS} API to exchange messages. More information can be found on the article written by \citeauthor{site:ros_dds}~\cite{site:ros_dds}.

\subsubsection{Navigation}
``Nav 2'' is the navigation system provided by \gls{ROS}. A developer can chose to use their own navigation system but the one developed by~\citeauthor{paper:navigation2} is widely tested and supported~\cite{paper:navigation2}. 

\subsection{Gazebo simulator}
Gazebo is an open-source simulator that allows simulating environments where robots are involved. Gazebo offers the ability to accurately and efficiently simulate populations of robots in complex indoor and outdoor environments. Moreover, there is the possibility to use different robot models using \glsfirst{SDF} file and to import Collada file into the simulated world. Gazebo is also expandable with plugins. One of those allows using \acrshort{ROS} to communicate with the robots inside the simulation. I used the Gazebo simulator to perform some tests before deploying the software in a real environment. 

\subsection{Git}
Git is a distributed version control system. It has been extensively used to store and share the source code and documentation of this project. In particular GitHub has been used, creating several repositories.

\section{System design and implementation}\label{sec:system_design_and_implementation}

\subsection{Hand gesture recognizer}
The first challenge to to solve was the design and the implementation of the hand gesture recognizer. In particular, two types of hand gestures was considered:
\begin{itemize}
    \item \textbf{Static hand gestures}: in which the hand doesn't move and only a snapshot of the position of the fingers is sufficient to recognize the gesture;
    \item \textbf{Dynamic hand gesture}: in which the hand performs a movement. In this case, a sequence of data is necessary to recognize the gesture. 
\end{itemize}
This diversity leads to two different neural networks to classify the hand gesture the user is doing.\\
The hand gesture recognizer was developed starting from the project of~\citeauthor{site:hand_gesture_base_repo}~\cite{site:hand_gesture_base_repo} to which several improvements were made. Before running the hand gesture detector, the user can choose in which mode running the program:
\begin{itemize}
    \item \textbf{operational};
    \item \textbf{learning};
    \item \textbf{macro}.
\end{itemize}

\subsubsection{Operational mode}
Within this mode the operator can do a sequence of hand gesture that will be translated in commands for the robot and will be sent to it through \acrshort{ROS}' communication system
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{thesis/images/systemArchitectureOperational.png}
    \caption{Data flow in operational mode.}
    \label{fig:system_architecture_operational}
\end{figure}

\subsubsection{Learning mode}
In this mode the user can choose to add a new gesture or enhance one already existing adding more data to the dataset. In both cases, a command line interface is used to interact with the user
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{thesis/images/systemArchitectureTraining.png}
    \caption{Data flow in learning mode.}
    \label{fig:system_architecture_learning}
\end{figure}

\subsubsection{Macro mode}
 This mode is similar the the operational but, instead of sending the command in real time to a robot, the user can choose if save a sequence of them in a text file or ``execute'' a previously saved macro's file sending its content to a robot.
\begin{figure}[H]
    \centering

    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{thesis/images/SystemArchitectureSaveMacro.png}
        \caption{Data flow when creating a new macro.}
        \label{fig:system_architecture_save_macro}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{thesis/images/SystemArchitectureMacroRunner.png}
        \caption{Data flow when running a macro.}
        \label{fig:system_architecture_run_macro}
    \end{subfigure}
    
    \caption{Data flows in macro mode.}
    \label{fig:data_flows}
\end{figure}

\subsubsection{Static hand gestures}
The set of static hand gestures chosen is the \gls{ASL} (in figure~\ref{fig:asl}), \textit{J} \textit{Z} excluded because they involve a movement. To discriminate between dynamic and static gestures, I decided to prioritize the dynamic ones because the user has to perform an action to activate them. Instead, with the static gestures, the hand doesn't move, and the recognizer will always return a prediction. Also the numbers have been excluded.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{thesis/images/asl.png}
    \caption{\glsdesc{ASL}~\parencite{img:asl}}\label{fig:asl}
\end{figure}

\paragraph{MediaPipe data}

\paragraph{Deep neural network}

\paragraph{Training}

\subsubsection{Dynamic hand gestures}

\paragraph{MediaPipe data}

\paragraph{Deep neural network}

\paragraph{Training}

\subsection{Integration with ROS}
\end{document}
