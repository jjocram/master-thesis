\documentclass[../thesis.tex]{subfiles}
\begin{document}
\chapter{System design and implementation}\label{cap:system-design-and-implementation}
The systems is composed of two components: a node that deals with the hand gesture recognition and a node representing the robot to be controlled. The tasks that the robot must perform are:
\begin{itemize}
    \item Pick up a parcel;
    \item Drop down a parcel;
    \item Go to a predetermined position.
\end{itemize}
A letter identified positions and parcels. In this way, the \glsfirst{ASL} is exploitable in order to simulate the identification of positions and parcels.\\
Starting from this assumptions a finite state automata can be used to check the correctness of the input. Figure~\ref{fig:automata_for_commands} shows a possible automata with this purpose. The alphabet is $\Sigma = \{[A-Z], go\, to, pick\, up, drop\, down\}$ and the states are:
\begin{itemize}
    \item \textbf{$q_0$}: robot without parcel; 
    \item \textbf{$q_1$}: robot waiting for parcel id; 
    \item \textbf{$q_2$}: robot waiting for a ``direction'' without a parcel;
    \item \textbf{$q_3$}: robot with a parcel;
    \item \textbf{$q_4$}: robot waiting for a ``direction'' with a parcel.
\end{itemize}

\begin{figure}[H]
    \centering
    \resizebox{0.8\textwidth}{!}{%
    \begin{tikzpicture}[node distance = 4cm, on grid]
        \node (q0) [state, initial, accepting] {$q_0$};
        \node (q1) [state, above right = of q0] {$q_1$};
        \node (q3) [state, right = of q1] {$q_3$};
        \node (q4) [state, below right = of q3] {$q_4$};
        \node (q2) [state, below right = of q0] {$q_2$};

        \path [-stealth, thick]
            (q0) edge node[below right] {pick up} (q1)
            (q1) edge node[auto] {[A-Z]} (q3)
            (q3) edge node[auto] {go to} (q4)
            (q4) edge [bend left, auto] node {[A-Z]} (q3)
            (q0) edge node[auto] {go to} (q2)
            (q2) edge [bend left, auto] node {[A-Z]} (q0)
            (q3) edge [in=90,out=120,above,distance=3cm, auto] node[above left] {drop down} (q0);
    \end{tikzpicture}%
}
    \caption{Automata diagram for commands.}\label{fig:automata_for_commands}
\end{figure}
\gls{ROS} and its communication layer (i.e. exchange of messages) has to be used for the node implementation in order to comply with the requirement of interoperability between robots. For this reason, the hand gesture recognizer must be encapsulated inside a \gls{ROS} node.

\section{Hand gesture recognizer}
Two types of hand gestures have been considerate:
\begin{itemize}
    \item \textbf{Static hand gestures}: in which the hand doesn't move and only a snapshot of the position of the fingers is sufficient to recognize the gesture;
    \item \textbf{Dynamic hand gesture}: in which the hand performs a movement. In this case, a sequence of data is necessary to recognize the gesture. 
\end{itemize}
This diversity leads to two different neural networks to classify the hand gesture the user is doing.

\subsection{Static hand gestures}

\subsubsection{MediaPipe data}

\subsubsection{Deep neural network}

\subsubsection{Training}

\subsection{Dynamic hand gestures}

\subsubsection{MediaPipe data}

\subsubsection{Deep neural network}

\subsubsection{Training}

\section{Integration with ROS}

\end{document}
